{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ab8e58-bd4c-48e2-8864-3fad54fdeedb",
   "metadata": {},
   "source": [
    "# **Activation Functions: Sigmoid, ReLU, and others**\n",
    "\n",
    "## Python Demonstration of Activation Functions\n",
    "\n",
    "1. **Sigmoid**\n",
    "2. **Tanh**\n",
    "3. **ReLU**\n",
    "4. **Leaky ReLU**\n",
    "5. **ELU (Exponential Linear Unit)**\n",
    "6. **Swish** (used in modern architectures like EfficientNet)\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Summary\n",
    "\n",
    "| Activation | Formula | Range | Differentiability | Common Usage |\n",
    "|------------|---------|-------|--------------------|---------------|\n",
    "| **Sigmoid** | $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ | (0, 1) | Yes | Binary classification |\n",
    "| **Tanh** | $$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$ | (-1, 1) | Yes | Hidden layers |\n",
    "| **ReLU** | $$ f(x) = \\max(0, x) $$ | [0, ∞) | No at x=0 | CNNs |\n",
    "| **Leaky ReLU** | $$ f(x) = \\max(0.01x, x) $$ | (-∞, ∞) | Yes | CNNs with dying ReLU problem |\n",
    "| **ELU** | $$ f(x) = x , if ( x > 0 )$$  , $$ \\alpha(e^x - 1), if x \\leq 0 $$ | (-α, ∞) | Yes | Deep nets |\n",
    "| **Swish** | $$ f(x) = x \\cdot \\sigma(x) $$ | (-0.28, ∞) | Yes | Modern DNNs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4362748-f32e-43f6-bd42-d91bd87fe6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.ffmpeg_path'] = r'C:\\ffmpeg\\bin\\ffmpeg.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813e89d1-57db-4317-87bc-826f9f040e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating animation for: relu\n",
      "Generating animation for: sigmoid\n",
      "Generating animation for: tanh\n",
      "Generating animation for: softplus\n",
      "Generating animation for: softsign\n",
      "Generating animation for: leaky_relu\n",
      "Generating animation for: elu\n",
      "Generating animation for: swish\n",
      "Generating animation for: mish\n",
      "Generating animation for: gelu\n",
      "Generating animation for: hard_sigmoid\n",
      "Generating animation for: linear\n",
      "All animations generated!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "\n",
    "# Activation functions\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def tanh(x): return np.tanh(x)\n",
    "def softplus(x): return np.log(1 + np.exp(x))\n",
    "def softsign(x): return x / (1 + np.abs(x))\n",
    "def leaky_relu(x, alpha=0.01): return np.where(x > 0, x, x * alpha)\n",
    "def elu(x, alpha=1.0): return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "def swish(x): return x * sigmoid(x)\n",
    "def mish(x): return x * np.tanh(softplus(x))\n",
    "def gelu(x): return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "def hard_sigmoid(x): return np.clip((x + 1) / 2, 0, 1)\n",
    "def linear(x): return x\n",
    "\n",
    "functions = {\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"tanh\": tanh,\n",
    "    \"softplus\": softplus,\n",
    "    \"softsign\": softsign,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"elu\": elu,\n",
    "    \"swish\": swish,\n",
    "    \"mish\": mish,\n",
    "    \"gelu\": gelu,\n",
    "    \"hard_sigmoid\": hard_sigmoid,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "# Create meshgrid\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z_input = X + Y\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"activation_videos\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create animation for each activation function\n",
    "for name, func in functions.items():\n",
    "    print(f\"Generating animation for: {name}\")\n",
    "    Z = func(Z_input)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    def update_view(angle, ax=ax, X=X, Y=Y, Z=Z):\n",
    "        ax.clear()\n",
    "        ax.plot_surface(X, Y, Z, cmap=cm.viridis)\n",
    "        ax.set_title(f\"{name.upper()} Activation Function\", fontsize=12)\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(f\"{name.upper()}(X + Y)\")\n",
    "        ax.view_init(30, angle)\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_view, frames=np.arange(0, 360, 4), interval=100)\n",
    "    output_path = os.path.join(output_dir, f\"{name}_activation.mp4\")\n",
    "    ani.save(output_path, writer='ffmpeg', fps=10)\n",
    "    plt.close()\n",
    "print(\"All animations generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acc0a6b-6bed-400b-bee7-c4c1ced7ad9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating plot for: relu\n",
      "Generating plot for: sigmoid\n",
      "Generating plot for: tanh\n",
      "Generating plot for: softplus\n",
      "Generating plot for: softsign\n",
      "Generating plot for: leaky_relu\n",
      "Generating plot for: elu\n",
      "Generating plot for: swish\n",
      "Generating plot for: mish\n",
      "Generating plot for: gelu\n",
      "Generating plot for: hard_sigmoid\n",
      "Generating plot for: linear\n",
      "All activation function plots saved in 'activation_plots/' folder.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Activation functions\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def tanh(x): return np.tanh(x)\n",
    "def softplus(x): return np.log(1 + np.exp(x))\n",
    "def softsign(x): return x / (1 + np.abs(x))\n",
    "def leaky_relu(x, alpha=0.01): return np.where(x > 0, x, x * alpha)\n",
    "def elu(x, alpha=1.0): return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "def swish(x): return x * sigmoid(x)\n",
    "def mish(x): return x * np.tanh(softplus(x))\n",
    "def gelu(x): return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "def hard_sigmoid(x): return np.clip((x + 1) / 2, 0, 1)\n",
    "def linear(x): return x\n",
    "\n",
    "functions = {\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid,\n",
    "    \"tanh\": tanh,\n",
    "    \"softplus\": softplus,\n",
    "    \"softsign\": softsign,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"elu\": elu,\n",
    "    \"swish\": swish,\n",
    "    \"mish\": mish,\n",
    "    \"gelu\": gelu,\n",
    "    \"hard_sigmoid\": hard_sigmoid,\n",
    "    \"linear\": linear\n",
    "}\n",
    "\n",
    "# Create x range\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"activation_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create and save static 2D plots\n",
    "for name, func in functions.items():\n",
    "    print(f\"Generating plot for: {name}\")\n",
    "    y = func(x)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(x, y, label=name.upper(), color=\"blue\")\n",
    "    plt.title(f\"{name.upper()} Activation Function\")\n",
    "    plt.xlabel(\"Input\")\n",
    "    plt.ylabel(\"Output\")\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{name}_activation.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "print(\"All activation function plots saved in 'activation_plots/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4b65bd-9421-4072-9a99-74dc10c5c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Mathematical Formula</th>\n",
       "      <th>Advantages</th>\n",
       "      <th>Disadvantages</th>\n",
       "      <th>Example Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReLU</td>\n",
       "      <td>f(x) = max(0, x)</td>\n",
       "      <td>Simple, fast, and effective; avoids vanishing ...</td>\n",
       "      <td>Can 'die' during training if inputs are always...</td>\n",
       "      <td>Input: [-2, 0, 3] → Output: [0, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>f(x) = 1 / (1 + exp(-x))</td>\n",
       "      <td>Good for binary classification; outputs in (0,1).</td>\n",
       "      <td>Saturates and kills gradients; centered at 0.5.</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [0.12, 0.5, 0.88]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tanh</td>\n",
       "      <td>f(x) = tanh(x)</td>\n",
       "      <td>Centered at zero; better than sigmoid in hidde...</td>\n",
       "      <td>Still suffers from saturation and vanishing gr...</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [-0.96, 0, 0.96]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Softmax</td>\n",
       "      <td>f(xᵢ) = exp(xᵢ) / sum(exp(xⱼ))</td>\n",
       "      <td>Outputs valid probability distributions.</td>\n",
       "      <td>Not suitable for hidden layers; soft competition.</td>\n",
       "      <td>Input: [2.0, 1.0, 0.1] → Output: [0.65, 0.24, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leaky ReLU</td>\n",
       "      <td>f(x) = x if x &gt; 0 else αx</td>\n",
       "      <td>Fixes dying ReLU by allowing small gradients w...</td>\n",
       "      <td>Still not zero-centered.</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [-0.02, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ELU</td>\n",
       "      <td>f(x) = x if x &gt;= 0 else α(exp(x)-1)</td>\n",
       "      <td>Negative values allow mean activation closer t...</td>\n",
       "      <td>Computationally more expensive than ReLU.</td>\n",
       "      <td>Input: [-1, 0, 1] → Output: [-0.63, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SELU</td>\n",
       "      <td>λ * (x if x &gt; 0 else α*(exp(x)-1))</td>\n",
       "      <td>Self-normalizing when used with correct initia...</td>\n",
       "      <td>Sensitive to network configuration and dropout.</td>\n",
       "      <td>Input: [-1, 0, 1] → Output: [-1.11, 0, 1.05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GELU</td>\n",
       "      <td>f(x) ≈ 0.5x(1 + tanh(√(2/π)*(x + 0.044715x³)))</td>\n",
       "      <td>Smooth and combines benefits of ReLU and sigmoid.</td>\n",
       "      <td>More computationally intensive.</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [-0.05, 0, 1.95]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Swish (SiLU)</td>\n",
       "      <td>f(x) = x * sigmoid(x)</td>\n",
       "      <td>Non-monotonic, smooth, improves model performa...</td>\n",
       "      <td>Heavier computation than ReLU.</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [-0.24, 0, 1.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mish</td>\n",
       "      <td>f(x) = x * tanh(softplus(x))</td>\n",
       "      <td>Improved performance in vision tasks, smooth a...</td>\n",
       "      <td>High computational cost.</td>\n",
       "      <td>Input: [-2, 0, 2] → Output: [-0.25, 0, 1.94]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation Function                            Mathematical Formula  \\\n",
       "0                ReLU                                f(x) = max(0, x)   \n",
       "1             Sigmoid                        f(x) = 1 / (1 + exp(-x))   \n",
       "2                Tanh                                  f(x) = tanh(x)   \n",
       "3             Softmax                  f(xᵢ) = exp(xᵢ) / sum(exp(xⱼ))   \n",
       "4          Leaky ReLU                       f(x) = x if x > 0 else αx   \n",
       "5                 ELU             f(x) = x if x >= 0 else α(exp(x)-1)   \n",
       "6                SELU              λ * (x if x > 0 else α*(exp(x)-1))   \n",
       "7                GELU  f(x) ≈ 0.5x(1 + tanh(√(2/π)*(x + 0.044715x³)))   \n",
       "8        Swish (SiLU)                           f(x) = x * sigmoid(x)   \n",
       "9                Mish                    f(x) = x * tanh(softplus(x))   \n",
       "\n",
       "                                          Advantages  \\\n",
       "0  Simple, fast, and effective; avoids vanishing ...   \n",
       "1  Good for binary classification; outputs in (0,1).   \n",
       "2  Centered at zero; better than sigmoid in hidde...   \n",
       "3           Outputs valid probability distributions.   \n",
       "4  Fixes dying ReLU by allowing small gradients w...   \n",
       "5  Negative values allow mean activation closer t...   \n",
       "6  Self-normalizing when used with correct initia...   \n",
       "7  Smooth and combines benefits of ReLU and sigmoid.   \n",
       "8  Non-monotonic, smooth, improves model performa...   \n",
       "9  Improved performance in vision tasks, smooth a...   \n",
       "\n",
       "                                       Disadvantages  \\\n",
       "0  Can 'die' during training if inputs are always...   \n",
       "1    Saturates and kills gradients; centered at 0.5.   \n",
       "2  Still suffers from saturation and vanishing gr...   \n",
       "3  Not suitable for hidden layers; soft competition.   \n",
       "4                           Still not zero-centered.   \n",
       "5          Computationally more expensive than ReLU.   \n",
       "6    Sensitive to network configuration and dropout.   \n",
       "7                    More computationally intensive.   \n",
       "8                     Heavier computation than ReLU.   \n",
       "9                           High computational cost.   \n",
       "\n",
       "                                      Example Output  \n",
       "0              Input: [-2, 0, 3] → Output: [0, 0, 3]  \n",
       "1      Input: [-2, 0, 2] → Output: [0.12, 0.5, 0.88]  \n",
       "2       Input: [-2, 0, 2] → Output: [-0.96, 0, 0.96]  \n",
       "3  Input: [2.0, 1.0, 0.1] → Output: [0.65, 0.24, ...  \n",
       "4          Input: [-2, 0, 2] → Output: [-0.02, 0, 2]  \n",
       "5          Input: [-1, 0, 1] → Output: [-0.63, 0, 1]  \n",
       "6       Input: [-1, 0, 1] → Output: [-1.11, 0, 1.05]  \n",
       "7       Input: [-2, 0, 2] → Output: [-0.05, 0, 1.95]  \n",
       "8       Input: [-2, 0, 2] → Output: [-0.24, 0, 1.76]  \n",
       "9       Input: [-2, 0, 2] → Output: [-0.25, 0, 1.94]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Previously created dataframe for activation functions\n",
    "activation_details = [\n",
    "    {\n",
    "        \"Function\": \"ReLU\",\n",
    "        \"Formula\": \"f(x) = max(0, x)\",\n",
    "        \"Pros\": \"Simple, fast, and effective; avoids vanishing gradients.\",\n",
    "        \"Cons\": \"Can 'die' during training if inputs are always negative.\",\n",
    "        \"Example\": \"Input: [-2, 0, 3] → Output: [0, 0, 3]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Sigmoid\",\n",
    "        \"Formula\": \"f(x) = 1 / (1 + exp(-x))\",\n",
    "        \"Pros\": \"Good for binary classification; outputs in (0,1).\",\n",
    "        \"Cons\": \"Saturates and kills gradients; centered at 0.5.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [0.12, 0.5, 0.88]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Tanh\",\n",
    "        \"Formula\": \"f(x) = tanh(x)\",\n",
    "        \"Pros\": \"Centered at zero; better than sigmoid in hidden layers.\",\n",
    "        \"Cons\": \"Still suffers from saturation and vanishing gradients.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.96, 0, 0.96]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Softmax\",\n",
    "        \"Formula\": \"f(xᵢ) = exp(xᵢ) / sum(exp(xⱼ))\",\n",
    "        \"Pros\": \"Outputs valid probability distributions.\",\n",
    "        \"Cons\": \"Not suitable for hidden layers; soft competition.\",\n",
    "        \"Example\": \"Input: [2.0, 1.0, 0.1] → Output: [0.65, 0.24, 0.11]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Leaky ReLU\",\n",
    "        \"Formula\": \"f(x) = x if x > 0 else αx\",\n",
    "        \"Pros\": \"Fixes dying ReLU by allowing small gradients when x < 0.\",\n",
    "        \"Cons\": \"Still not zero-centered.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.02, 0, 2]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"ELU\",\n",
    "        \"Formula\": \"f(x) = x if x >= 0 else α(exp(x)-1)\",\n",
    "        \"Pros\": \"Negative values allow mean activation closer to zero.\",\n",
    "        \"Cons\": \"Computationally more expensive than ReLU.\",\n",
    "        \"Example\": \"Input: [-1, 0, 1] → Output: [-0.63, 0, 1]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"SELU\",\n",
    "        \"Formula\": \"λ * (x if x > 0 else α*(exp(x)-1))\",\n",
    "        \"Pros\": \"Self-normalizing when used with correct initialization.\",\n",
    "        \"Cons\": \"Sensitive to network configuration and dropout.\",\n",
    "        \"Example\": \"Input: [-1, 0, 1] → Output: [-1.11, 0, 1.05]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"GELU\",\n",
    "        \"Formula\": \"f(x) ≈ 0.5x(1 + tanh(√(2/π)*(x + 0.044715x³)))\",\n",
    "        \"Pros\": \"Smooth and combines benefits of ReLU and sigmoid.\",\n",
    "        \"Cons\": \"More computationally intensive.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.05, 0, 1.95]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Swish (SiLU)\",\n",
    "        \"Formula\": \"f(x) = x * sigmoid(x)\",\n",
    "        \"Pros\": \"Non-monotonic, smooth, improves model performance.\",\n",
    "        \"Cons\": \"Heavier computation than ReLU.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.24, 0, 1.76]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Mish\",\n",
    "        \"Formula\": \"f(x) = x * tanh(softplus(x))\",\n",
    "        \"Pros\": \"Improved performance in vision tasks, smooth activation.\",\n",
    "        \"Cons\": \"High computational cost.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.25, 0, 1.94]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Softplus\",\n",
    "        \"Formula\": \"f(x) = log(1 + exp(x))\",\n",
    "        \"Pros\": \"Smooth version of ReLU.\",\n",
    "        \"Cons\": \"Not sparse; all neurons fire.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [0.13, 0.69, 2.13]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Softsign\",\n",
    "        \"Formula\": \"f(x) = x / (1 + |x|)\",\n",
    "        \"Pros\": \"Slower saturation than tanh/sigmoid.\",\n",
    "        \"Cons\": \"Weaker gradients for large values.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-0.67, 0, 0.67]\"\n",
    "    },\n",
    "    {\n",
    "        \"Function\": \"Linear\",\n",
    "        \"Formula\": \"f(x) = x\",\n",
    "        \"Pros\": \"Used in output layers for regression.\",\n",
    "        \"Cons\": \"No non-linearity; can't learn complex patterns.\",\n",
    "        \"Example\": \"Input: [-2, 0, 2] → Output: [-2, 0, 2]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(activation_details)\n",
    "df.columns = ['Activation Function', 'Mathematical Formula', 'Advantages', 'Disadvantages', 'Example Output']\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd88308-f219-4d35-907a-44dcf8f41909",
   "metadata": {},
   "source": [
    "# Activation Functions Overview\n",
    "\n",
    "| Activation Function | Mathematical Formula | Advantages | Disadvantages | Example Output |\n",
    "|---------------------|----------------------|------------|----------------|----------------|\n",
    "| ReLU | $f(x) = max(0, x)$ | Simple, fast, and effective; avoids vanishing gradients. | Can 'die' during training if inputs are always negative. | `Input: [-2, 0, 2]`→ `Output: [0, 0, 2]` |\n",
    "| Sigmoid | $f(x) = \\frac{1}{1 + exp(-x)} $ | Good for binary classification; outputs in $(0,1)$. | Saturates and kills gradients; centered at 0.5. | `Input: [-2, 0, 2]` → `Output: [0.12, 0.5, 0.88]` |\n",
    "| Tanh | $f(x) = tanh(x)$ | Centered at zero; better than sigmoid in hidden layers. | Still suffers from saturation and vanishing gradients. | `Input: [-2, 0, 2]` → `Output: [-0.96, 0, 0.96]` |\n",
    "| Softmax | $f(xᵢ) = \\frac{exp(xᵢ)} {\\sum(exp(xⱼ))}$ | Outputs valid probability distributions. | Not suitable for hidden layers; soft competition. | `Input: [2.0, 1.0, 0.1]` → `Output: [0.65, 0.24, 0.11]` |\n",
    "| Leaky ReLU | $f(x) = x$ if $x > 0$ else $αx$ | Fixes dying ReLU by allowing small gradients when $x < 0$. | Still not zero-centered. | `Input: [-2, 0, 2]` → `Output: [-0.02, 0, 2]` |\n",
    "| ELU | $f(x) = x$ if $x >= 0$ else $α(exp(x)-1)$ | Negative values allow mean activation closer to zero. | Computationally more expensive than ReLU. | `Input: [-1, 0, 1]` → `Output: [-0.63, 0, 1]` |\n",
    "| SELU | $λ * x$ if $x > 0$ else $λ*α*(exp(x)-1)$ | Self-normalizing when used with correct initialization. | Sensitive to network configuration and dropout. | `Input: [-1, 0, 1]` → `Output: [-1.11, 0, 1.05]` |\n",
    "| GELU | $f(x) ≈ 0.5x(1 + tanh(√(2/π)*(x + 0.044715x³)))$ | Smooth and combines benefits of ReLU and sigmoid. | More computationally intensive. | `Input: [-2, 0, 2]` → `Output: [-0.05, 0, 1.95]` |\n",
    "| Swish (SiLU) | $f(x) = x * sigmoid(x)$ | Non-monotonic, smooth, improves model performance. | Heavier computation than ReLU. | `Input: [-2, 0, 2]` → `Output: [-0.24, 0, 1.76]` |\n",
    "| Mish | $f(x) = x * tanh(softplus(x))$ | Improved performance in vision tasks, smooth activation. | High computational cost. | `Input: [-2, 0, 2]` → `Output: [-0.25, 0, 1.94]` |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
