{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# CAP 781\n",
    "# Bagging, Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees as a high-variance model\n",
    "\n",
    "* `Unpruned decision trees`, which are constructed by recursive partitioning and greedy search, can be `highly variable`. A small change in the training data could greatly distort the tree: the series of splits could be very different. In contrast, a procedure with **low variability** will yield **similar results** if applied\n",
    "repeatedly to distinct data sets.\n",
    "\n",
    "* Question: How can we reduce the variability of decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging \n",
    "\n",
    "* Suppose we have a random sample  $\\{Z_1,\\ldots, Z_n\\}$\n",
    "from some distribution such as:\n",
    "\n",
    "$$\n",
    "\\{Z_i\\}_{i=1}^{n} \\stackrel{iid}{\\sim} (\\mu,\\sigma^2),\n",
    "$$\n",
    " \n",
    "* with mean $E(Z_i)=\\mu$ and variance $Var(Z_i)=\\sigma^2$, for $i=1,\\ldots,n$.\n",
    "\n",
    "* Let $\\bar{Z}=\\frac{ \\sum_{i=1}^{n}Z_i}{n}$ be the sample mean.\n",
    "Since the observations are **independent** in a random sample, we have:\n",
    "\n",
    "$$\n",
    "E(\\bar{Z})=\\mu  \\\\\n",
    "Var(\\bar{Z}) = \\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "* In words, the **variance of the average** is **less** than the variance of the sample elements, i.e., $Var(\\bar{Z})\\leq Var(Z_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging \n",
    "\n",
    "* Hence, a natural way to **reduce the variance** of a statistical learning method is to **take many training samples** (from the population of interest), build a separate prediction model using each training set, and **average the resulting predictions**.\n",
    "\n",
    "* However, in practice, we have access to **only one training data**, **not many**. How can we solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging) Algorithm\n",
    "\n",
    "\n",
    "* At this point, we can apply `Bootstrap aggregation` or `bagging` which was proposed by Breiman (1996).\n",
    "   * Let $\\mathcal{D} = \\{(\\mathbf{x}_{1}, y_{1}), (\\mathbf{x}_{2}, y_{2}), \\ldots, (\\mathbf{x}_{n}, y_{n})\\}$ denote the **training data**.\n",
    "   * Furthermore, consider a **learning algorithm** that takes the training data as its input and produces a prediction outcome such as $f(\\mathcal{D})$.\n",
    "   * We can draw a bootstrap sample by sampling from the training data **with replacement** such as: $\\mathcal{D}^{*b} = \\{(\\mathbf{x}_{1}^{*b}, y_{1}^{*b}),   (\\mathbf{x}_{2}^{*b}, y_{2}^{*b}), \\ldots, (\\mathbf{x}_{n}^{*b}, y_{n}^{*b})\\}$ and repeat the procedure for $b = 1, \\ldots,B$.\n",
    "\n",
    "\n",
    "```\n",
    "* An example of a bootstrap samples. Suppose this is your original dataset: [1,2,3,4]\n",
    "\n",
    "   * A bootstrap sample drawn with replacement: [1,1,3,4]\n",
    "\n",
    "   * A bootstrap sample drawn with replacement: [3,2,2,2]\n",
    "\n",
    "   * A bootstrap sample drawn with replacement: [1,2,4,4]\n",
    "```   \n",
    "   \n",
    "   * Then, we can apply the learning algorithm to the each bootstrap sample to learn the structure of the tree $f(\\mathcal{D}^{*b})$, for $b = 1, \\ldots,B$.\n",
    "\n",
    "![](images/bootstrap.png)\n",
    "\n",
    "   * For **classification** of a given a new observation ${x}_{n+1}$,  let $f(\\mathcal{D}^{*b})$ be the predicted class of new observation by $bth$ learning algorithm. Then the\n",
    "boostrap aggregated (bagged) prediction is given by:\n",
    "   \n",
    "\n",
    "   \\begin{equation}\n",
    "    \\hat{y}^{bag}_{n+1}= \\underset{(l \\in \\{1,\\ldots,k,\\ldots,K \\})}{argmax} \\sum_{b=1}^{B} I(f(\\mathcal{D}^{*b})=c_l). \\nonumber\n",
    "   \\end{equation}\n",
    "\n",
    "   * Here, we record the class predicted by each of the algorithms and take a `majority vote`: the overall prediction is the **most commonly occurring class**\n",
    "among the $B$ predictions.\n",
    "* In Breiman (1996a) $B = 50$ was used in all numeric examples, but there is no particular reason for this choice. If the learning algorithm is\n",
    "fast to compute then one can certainly use a larger B (say B = 100, 200, 300, 400, 500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Classification Trees\n",
    "\n",
    "* `Bagging` is a general-purpose procedure for **reducing the variability**, in other words, **improving instability** of a statistical learning method. However, `Bagging` is particularly useful for decision trees.\n",
    "\n",
    "* To apply bagging to classification trees, we simply construct $B$ classification trees using $B$ bootstrapped training sets, apply the equation (3) for the prediction of new observations.\n",
    "\n",
    "* These trees are grown  deep and are not pruned in general. Hence each individual tree has **high variance**. **Avereging** these $B$ trees **reduces the variance**.\n",
    "\n",
    "![](images/bagging.png)\n",
    "\n",
    "* The literature has shown that **bagging** demonstrated impressive **improvements in prediction accuracy** of decision trees by `ensembling` multiple trees into a single procedure.\n",
    "* In machine learning world, `bagging` is also called as an `ensemble` learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests: Intuition\n",
    "\n",
    "* Remember that if $\\{Z_i\\}_{i=1}^{n} \\stackrel{idd}{\\sim} (\\mu,\\sigma^2)$ is\n",
    "a random sample from some distribution with mean $E(Z_i)=\\mu$ and variance $Var(Z_i)=\\sigma^2$, for $i=1,\\ldots,n$. \n",
    "* Then, we have:\n",
    "\n",
    "$$\n",
    "E(\\bar{Z})=\\mu  \\\\\n",
    "Var(\\bar{Z}) = \\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "* However, if the `pairwise correlation` between any two observations $Corr(Z_i,Z_j)=\\rho$ exists, for $i \\neq j$,\n",
    "then:\n",
    "\n",
    "$$\n",
    "Var(\\bar{Z}) = \\rho \\sigma^2 +  \\frac{1-\\rho}{n}\\sigma^2.\n",
    "$$\n",
    "\n",
    "* In words, correlation between samples **limits the variance-reducing effect of averaging**.\n",
    "\n",
    "* In a similar fashion, since each tree is built using a bootstrap sample from the original training\n",
    "data, the trees in the **bagged ensemble will be somewhat correlated**.\n",
    "\n",
    "* If we can **reduce correlation between the trees**, the trees will be more diverse and averaging can further improve the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests: Definition\n",
    "\n",
    "* Random forests is an ensemble learning classifier invented by Breiman (Breiman, 2001) and provide an **improvement over bagged trees** through `decorrelating` the trees.\n",
    "*  Breiman(2001) aimed to reduce the correlations between the trees by using a **randomly selected** $m$ features from full set of $d$ features when building splits of decision tree, which can further enhance the variance reduction capability of bagging. (Note that random selection of features at the node level, not tree level).\n",
    "\n",
    "![](images/rf.png)\n",
    "\n",
    "* We typically choose $m = \\sqrt{d}$--that is, the number of predictors considered at each split is approximately equal to the square root of the total number of features.\n",
    "* If a random forest is built using $m = d$, then this amounts to simply bagging.\n",
    "* Note that **Bagging** uses a **randomized method**, namely bootstrap, to generate a collection of classification trees. **Random forests** put another level of **randomization** on top of bagging by using a randomly selected a subset of predictors to construct each classification tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pros and Cons of Random Forests\n",
    "\n",
    "Random forests remain a popular machine learning algorithm:\n",
    "\n",
    "* They require **little data preparation** (no rescaling, handle continuous and discrete features, work well for classification and regression).\n",
    "* They are one of the best performing off-the-shelf classifiers **without heavy tuning** of hyperparameters\n",
    "\n",
    "Their main disadvantages are that:\n",
    "\n",
    "* They are **not interpretable**.\n",
    "* They are **slower** than decision trees because we are fitting multiple trees but can easily parallelize training because all trees are independent of each other.\n",
    "* They require **more memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., Tibshirani, R., James, G., Witten, D., and Tibshirani, R. (2021). Statistical learning. An introduction to statistical learning: with applications in R. https://www.statlearning.com/.\n",
    "- MÃ¼ller, A. C., and Guido, S. (2016). Introduction to machine learning with Python: A guide for data scientists. O'Reilly Media, Inc.\n",
    "- Fan, J., Li, R., Zhang, C.H. and Zou, H., 2020. Statistical foundations of data science. CRC press.\n",
    "- Wade, Corey, and Kevin Glynn. Hands-On Gradient Boosting with XGBoost and scikit-learn: Perform accessible machine learning and extreme gradient boosting with Python. Packt Publishing Ltd, 2020.\n",
    "- Greenwell, B. M. (2022). Tree-based Methods for Statistical Learning in R. CRC Press.\n",
    "- https://github.com/kuleshov/cornell-cs5785-2020-applied-ml/tree/main/notebooks\n",
    "- https://ubc-cs.github.io/cpsc330/lectures/11_ensembles.html\n",
    "- https://ubc-cs.github.io/cpsc330/lectures/12_feat-importances.html\n",
    "- https://lewtun.github.io/hepml/lesson02_random-forest-deep-dive/\n",
    "- https://inria.github.io/scikit-learn-mooc/trees/trees_module_intro.html\n",
    "- https://www-users.cse.umn.edu/~kumar001/dmbook/slides/chap3_basic_classification.pdf\n",
    "- https://www-users.cse.umn.edu/~kumar001/dmbook/slides/chap3_overfitting.pdf\n",
    "- https://www-users.cse.umn.edu/~kumar001/dmbook/slides/chap4_ensemble.pdf\n",
    "- https://stats.stackexchange.com/questions/357990/in-random-forest-why-is-a-random-subset-of-features-chosen-at-the-node-level-ra\n",
    "- https://stats.stackexchange.com/questions/285006/random-forest-advantages-disadvantages-of-selecting-randomly-subset-features-fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "session_info        1.0.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "anyio                       NA\n",
       "appnope                     0.1.2\n",
       "attr                        21.4.0\n",
       "babel                       2.9.1\n",
       "backcall                    0.2.0\n",
       "brotli                      1.0.9\n",
       "certifi                     2021.10.08\n",
       "chardet                     4.0.0\n",
       "charset_normalizer          2.0.4\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "entrypoints                 0.3\n",
       "google                      NA\n",
       "idna                        3.3\n",
       "ipykernel                   6.4.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.1\n",
       "jinja2                      2.11.3\n",
       "json5                       NA\n",
       "jsonschema                  3.2.0\n",
       "jupyter_server              1.4.1\n",
       "jupyterlab_server           2.10.2\n",
       "markupsafe                  1.1.1\n",
       "mpl_toolkits                NA\n",
       "nbclassic                   NA\n",
       "nbformat                    5.1.3\n",
       "packaging                   21.3\n",
       "parso                       0.8.3\n",
       "pexpect                     4.8.0\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prometheus_client           NA\n",
       "prompt_toolkit              3.0.20\n",
       "ptyprocess                  0.7.0\n",
       "pvectorc                    NA\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pyrsistent                  NA\n",
       "requests                    2.27.1\n",
       "send2trash                  NA\n",
       "six                         1.16.0\n",
       "sniffio                     1.2.0\n",
       "socks                       1.7.1\n",
       "sphinxcontrib               NA\n",
       "storemagic                  NA\n",
       "tornado                     6.1\n",
       "traitlets                   5.1.1\n",
       "uritemplate                 4.1.1\n",
       "urllib3                     1.26.8\n",
       "wcwidth                     0.2.5\n",
       "zmq                         22.3.0\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.31.1\n",
       "jupyter_client      6.1.12\n",
       "jupyter_core        4.9.1\n",
       "jupyterlab          3.2.1\n",
       "notebook            6.4.6\n",
       "-----\n",
       "Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]\n",
       "macOS-10.15.7-x86_64-i386-64bit\n",
       "-----\n",
       "Session information updated at 2023-05-02 13:25\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-ode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "rise": {
   "controlsTutorial": false,
   "height": 900,
   "help": false,
   "margin": 0,
   "maxScale": 2,
   "minScale": 0.2,
   "progress": true,
   "scroll": true,
   "theme": "simple",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
